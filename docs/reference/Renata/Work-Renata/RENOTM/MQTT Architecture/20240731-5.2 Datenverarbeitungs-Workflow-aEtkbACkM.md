# 5.2 Datenverarbeitungs-Workflow

## Einleitung

Workflow umfasst alle Schritte von der Datenerfassung an der Quelle bis zur Analyse und Visualisierung in unseren Geschäftsanwendungen. Ziel ist es, Echtzeiteinblicke in unsere Produktionsprozesse zu gewinnen und datengesteuerte Entscheidungen zu ermöglichen.

## Übersicht des Workflows

1. Datenerfassung
2. Edge-Verarbeitung
3. MQTT-Vermittlung
4. Cloud-Ingestion
5. Streaming-Verarbeitung
6. Datenspeicherung
7. Batch-Verarbeitung
8. Analyse und Visualisierung

## Detaillierte Workflow-Beschreibung

### 1. Datenerfassung

- Quellen: 
  - Siemens S7-1500 SPS
  - HBM QuantumX Datenerfassungssysteme
  - Endress+Hauser Sensoren

- Erfasste Daten:
  - Prozessparameter (Temperatur, Druck, Flussrate)
  - Maschinenstatusdaten
  - Qualitätsmesswerte
  - Produktionszähler

- Erfassungsfrequenz:
  - Kritische Prozessdaten: 100 ms
  - Standardtelemetrie: 1 s
  - Statusupdates: 5 s

### 2. Edge-Verarbeitung

- Hardware: Moxa ThingsPro Gateway
- Software: NanoMQ, Azure IoT Edge Runtime

Verarbeitungsschritte:
a) Datenvalidierung und -bereinigung
   - Entfernung von Ausreißern
   - Überprüfung auf fehlende Werte
   - Formatierung und Standardisierung

b) Lokale Aggregation
   - Berechnung von Durchschnittswerten über definierte Zeitfenster
   - Ermittlung von Min/Max-Werten

c) Regelbasierte Anomalieerkennung
   - Überprüfung auf Grenzwertüberschreitungen
   - Erkennung von ungewöhnlichen Mustern

d) Datenkomprimierung
   - Anwendung von Lossless-Kompressionsalgorithmen

e) Protokollumwandlung
   - Konvertierung von Modbus/OPC UA zu MQTT

### 3. MQTT-Vermittlung

- Broker: EMQX Enterprise 4.4.x auf AKS
- Topics: Strukturiert nach unseren Naming-Konventionen (siehe 2.3.1)

Verarbeitungsschritte:
a) Nachrichtenvalidierung
   - Überprüfung des Nachrichtenformats
   - Authentifizierung und Autorisierung

b) Topic-basiertes Routing
   - Weiterleitung basierend auf konfigurierten Regeln

c) Nachrichtentransformation
   - Anwendung der EMQX Rule Engine für einfache Transformationen

d) QoS-Management
   - Sicherstellung der korrekten Nachrichtenzustellung gemäß QoS-Level

### 4. Cloud-Ingestion

- Dienst: Azure IoT Hub
- Konfiguration: S1 Standard Tier, 4 Partitionen

Verarbeitungsschritte:
a) Geräteauthentifizierung
   - Überprüfung von Gerätezertifikaten

b) Nachrichtendecodierung
   - Deserialisierung der MQTT-Payload

c) Nachrichtenrouting
   - Weiterleitung an Azure Event Hubs basierend auf Nachrichteninhalt

d) Gerätezwillingsaktualisierung
   - Aktualisierung von Gerätemetadaten und -zuständen

### 5. Streaming-Verarbeitung

- Dienst: Azure Stream Analytics
- Konfiguration: 6 Streaming Units

Verarbeitungsschritte:
a) Echtzeitaggregation
   - Berechnung von gleitenden Durchschnitten
   - Erstellung von Zeitfenstern für Analysen

b) Komplexe Ereignisverarbeitung
   - Erkennung von Mustern über mehrere Datenströme hinweg
   - Auslösung von Alarmen basierend auf definierten Bedingungen

c) Anreicherung
   - Zusammenführung von Echtzeitdaten mit statischen Referenzdaten

d) OEE-Berechnung (Overall Equipment Effectiveness)
   - Echtzeitberechnung von Verfügbarkeit, Leistung und Qualität

e) Datenfilterung
   - Entfernung irrelevanter oder redundanter Daten

### 6. Datenspeicherung

- Dienste: 
  - Azure Data Lake Storage Gen2 (für Rohdaten)
  - Azure Cosmos DB (für verarbeitete Daten)

Verarbeitungsschritte:
a) Datenpartitionierung
   - Aufteilung der Daten nach Datum und Produktionslinie

b) Datenformatierung
   - Konvertierung in Parquet-Format für effiziente Analyse

c) Indexierung
   - Erstellung von Indizes für häufig abgefragte Felder

d) Datenkompression
   - Anwendung von Snappy-Kompression für optimierte Speichernutzung

### 7. Batch-Verarbeitung

- Dienst: Azure Databricks

Verarbeitungsschritte:
a) Datennormalisierung
   - Standardisierung von Einheiten und Formaten

b) Feature-Engineering
   - Erstellung abgeleiteter Variablen für ML-Modelle

c) Historische Analysen
   - Berechnung von Langzeittrends und saisonalen Mustern

d) Datenqualitätsprüfungen
   - Durchführung umfassender Konsistenz- und Vollständigkeitsprüfungen

e) ML-Modelltraining
   - Regelmäßiges Training von prädiktiven Wartungsmodellen

### 8. Analyse und Visualisierung

- Dienste:
  - Power BI für Dashboards und Berichte
  - Azure Machine Learning für prädiktive Analysen

Verarbeitungsschritte:
a) Datenvisualisierung
   - Erstellung interaktiver Dashboards für verschiedene Benutzergruppen

b) Ad-hoc-Analysen
   - Bereitstellung von Self-Service-BI-Tools für Fachexperten

c) Prädiktive Wartung
   - Anwendung von ML-Modellen zur Vorhersage von Maschinenausfällen

d) Prozessoptimierung
   - Identifikation von Verbesserungspotentialen basierend auf historischen Daten

e) Alarmierung und Benachrichtigung
   - Konfiguration von Echtzeit-Alerts für kritische KPIs

## Datenfluss und Latenz

- Edge zu MQTT-Broker: < 50 ms
- MQTT-Broker zu IoT Hub: < 100 ms
- IoT Hub zu Stream Analytics: < 500 ms
- Stream Analytics zu Data Lake: < 1 s
- Gesamtlatenz von Sensor zu Dashboard: < 2 s

## Fehlerbehandlung und Resilienz

1. Implementierung von Retry-Logik auf allen Ebenen
2. Dead-Letter-Queues für nicht verarbeitbare Nachrichten
3. Datenpufferung an Edge-Geräten bei Netzwerkausfällen
4. Automatische Failover-Mechanismen in Azure-Diensten

## Skalierbarkeit

- Horizontale Skalierung von EMQX-Nodes bei erhöhtem Nachrichtenaufkommen
- Automatische Skalierung von Azure Stream Analytics basierend auf Eingabedurchsatz
- Partitionierung von Daten in Data Lake für parallele Verarbeitung

## Sicherheit und Compliance

1. Durchgängige Datenverschlüsselung (in Transit und at Rest)
2. Rollenbasierte Zugriffskontrolle (RBAC) für alle Azure-Ressourcen
3. Regelmäßige Sicherheitsaudits und Penetrationstests
4. Einhaltung von DSGVO-Anforderungen durch Daten-Tagging und Löschrichtlinien

## Monitoring und Logging

- Zentralisiertes Logging mit Azure Log Analytics
- End-to-End-Überwachung mit Azure Application Insights
- Anpassbare Dashboards für Systemzustand und Performance
- Automatisierte Alerts bei Anomalien oder Leistungseinbrüchen

## Optimierungspotenziale

1. Implementierung von Auto-Tuning für Stream Analytics-Jobs
2. Erweiterung der Edge-Analytics-Fähigkeiten zur Reduzierung der Cloud-Datenübertragung
3. Einsatz von Delta Lake für verbesserte Datenverwaltung im Data Lake
4. Integration von AutoML für kontinuierliche Verbesserung der prädiktiven Modelle

## Fazit

Der hier beschriebene Datenverarbeitungs-Workflow ermöglicht uns, den gesamten Datenlebenszyklus von der Erfassung bis zur Analyse effizient zu managen und wertvolle Erkenntnisse aus unseren Produktionsdaten zu gewinnen. Durch die Kombination von Edge-Computing, Cloud-Verarbeitung und fortschrittlichen Analysetechniken haben wir eine skalierbare und zukunftssichere Lösung, die es uns ermöglicht, agil auf Marktanforderungen zu reagieren und unsere Wettbewerbsfähigkeit kontinuierlich zu verbessern.